{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_folders = []\n",
    "    folders = os.listdir(path)\n",
    "    for folder in folders[1:]:\n",
    "        img_folders.append(folder)\n",
    "        \n",
    "        label = folder.split(\"_\")[0]\n",
    "        labels.append(label)\n",
    "        \n",
    "    img_folders = np.array(img_folders)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    training_folders, testing_folders, y_train, y_test = train_test_split(img_folders, labels, test_size=0.3, random_state=0, stratify=labels)\n",
    "    print(training_folders.shape,testing_folder.shape)\n",
    "    \n",
    "    np.save(\"training_folders\",training_folders)\n",
    "    np.save(\"testing_folders\",testing_folders)\n",
    "    np.save(\"training_labels\",y_train)\n",
    "    np.save(\"testing_labels\",y_test)\n",
    "    \n",
    "    return training_folders, testing_folder, y_train, y_test\n",
    "\n",
    "def find_features(path,training_folders):\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    descriptor_list = []\n",
    "    for folder in training_folders:\n",
    "        file_path = path + folder\n",
    "        images = os.listdir(file_path)\n",
    "        video_descriptors_list = []\n",
    "        for img in images : \n",
    "            img_path = file_path + \"/\" + img\n",
    "            image = cv.imread(img_path,0)\n",
    "            kp,des = sift.detectAndCompute(image, None)\n",
    "            video_descriptors_list.append(des)\n",
    "        descriptor_list.append(video_descriptors_list)\n",
    "    \n",
    "    descriptor_list = np.array(descriptor_list)\n",
    "    print(type(descriptor_list[0]),type(descriptor_list[0][0]))\n",
    "    \n",
    "    np.save(\"./Data/training_descriptors\",descriptor_list)\n",
    "    print(descriptor_list.shape)\n",
    "\n",
    "def cluster(descriptor_stack):\n",
    "    kmeans_obj = KMeans(n_clusters = 20)\n",
    "    clustered_objects = kmeans_obj.fit_predict(descriptor_stack)\n",
    "    clustered_objects = np.array(clustered_objects)\n",
    "    np.save(\"clustered_objects\",clustered_objects)\n",
    "    return clustered_objects,kmeans_obj\n",
    "    \n",
    "def stack_vertically(descriptor_list):\n",
    "    descriptor_stack = []\n",
    "    for i in range(descriptor_list.shape[0]):\n",
    "#         print(descriptor_list[i].shape)\n",
    "        for j in range(descriptor_list[i].shape[0]):\n",
    "#             print(descriptor_list[i][j].shape)\n",
    "#             print(i,j)\n",
    "            if descriptor_list[i][j] is not None:\n",
    "                descriptor_stack.extend(descriptor_list[i][j])\n",
    "    \n",
    "    descriptor_stack = np.array(descriptor_stack)\n",
    "    np.save(\"./Data/descriptor_stack\",descriptor_stack)\n",
    "    return np.array(descriptor_stack)\n",
    "\n",
    "def create_bovw_histogram(clustered_objects,training_folders,descriptor_list,path):\n",
    "    \n",
    "    histogram = np.array([np.zeros(20) for i in range(training_folders.shape[0])])\n",
    "    \n",
    "    for i in range(descriptor_list.shape[0]):\n",
    "        for j in range(descriptor_list[i].shape[0]):\n",
    "            for k in range(descriptor_list[i][j].shape[0]):\n",
    "                index = clustered_objects[i + j + k]\n",
    "                \n",
    "                histogram[i][index] += 1\n",
    "    \n",
    "    np.save(\"./Data/histogram\",histogram)\n",
    "    return histogram\n",
    "\n",
    "def normalize_histogram(histogram):\n",
    "    scale = StandardScaler().fit(histogram)\n",
    "    standardized_histogram = scale.transform(histogram)\n",
    "    \n",
    "    standardized_histogram = np.array(standardized_histogram)\n",
    "    \n",
    "    np.save(\"standardized_histogram\",standardized_histogram)\n",
    "    return standardized_histogram,scale\n",
    "\n",
    "def train_bovw(histogram,training_labels,clf):\n",
    "    clf.fit(histogram, training_labels)\n",
    "    return clf\n",
    "\n",
    "\n",
    "    \n",
    "def recognize_test_videos(testing_label,testing_folder,path,clf,scale,kmeans_obj):\n",
    "    test_histogram = [np.zeros(20) for i in range(testing_folder.shape[0])]\n",
    "\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for i in range(testing_folder.shape[0]):\n",
    "        folder = testing_folder[i]\n",
    "        file_path = path + folder\n",
    "        images = os.listdir(file_path)\n",
    "        \n",
    "#         test_histogram = [0 for j in range(20)]\n",
    "        for image in images : \n",
    "            img_path = file_path + \"/\" + img\n",
    "            image = cv.imread(img_path,0)\n",
    "            kp,des = sift.detectAndCompute(image, None)\n",
    "            test_clusters = kmeans_obj.predict(des)\n",
    "            \n",
    "            for each in test_clusters:\n",
    "                test_histogram[i][each] += 1\n",
    "        \n",
    "        test_histogram[i] = scale.transform(test_histogram[i])\n",
    "    \n",
    "    score = clf.score(test_histogram,testing_label)\n",
    "    print(score*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3009,) (1290,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1f623e412d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtraining_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# find_features(path,training_folders)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdescriptor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/training_descriptors.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdescriptor_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_vertically\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 421\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"./Data/Frames/\"\n",
    "training_folders, testing_folder, y_train, y_test = load_data(path)\n",
    "# find_features(path,training_folders)\n",
    "descriptor_list = np.load(\"./Data/training_descriptors.npy\")\n",
    "\n",
    "descriptor_stack = stack_vertically(descriptor_list)\n",
    "# descriptor_stack = np.load(\"./Data/descriptor_stack.npy\")\n",
    "\n",
    "\n",
    "#clustering\n",
    "clustered_objects,kmeans_obj = cluster(descriptor_stack)\n",
    "\n",
    "#creating visual vocabulary\n",
    "histogram = create_bovw_histogram(clustered_objects,training_folders,descriptor_list,path)\n",
    "\n",
    "#normalizing the histogram\n",
    "standardized_histogram, scale = normalize_histogram(histogram)\n",
    "\n",
    "# Training\n",
    "clf  = SVC()\n",
    "clf = train_bovw(standardized_histogram,y_train,clf)\n",
    "\n",
    "#Testing\n",
    "predicted_labels = recognize_test_videos(y_test,testing_folder,path,clf,scale,kmeans_obj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GestureRecognition",
   "language": "python",
   "name": "gesturerecognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
